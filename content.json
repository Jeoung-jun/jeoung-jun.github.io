{"posts":[{"title":"my new post","text":"개요 첫번째 글을 작성함 소스코드123import padnas as pddata = pd.read_csv('data.csv')data.head()","link":"/2023/02/21/my-new-post/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post 새로운 md 파일을 만들기 위해서는 아래와 같이 작성 1$ hexo new &quot;My New Post&quot; More info: Writing 미리 변화를 확인하기 위해서 로컬 서버를 사용해서 확인함 Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy generate 와 deploy 를 모두 실행하면 블로그에 배포가 됨 More info: Deployment","link":"/2023/02/21/introduction/"},{"title":"회귀 분류 정리","text":"회귀 모형 수식 $Y= W_1 \\times X_1 + W_2 \\times X_2 +W_3 \\times X_3 +W_4 \\times X_4 + …$ $X$ : 피처 $W$ : 회귀 계수 과적합 해결 1. 큰 회귀 계수의 예측 영향력을 감소시키려고 회귀 계수값을 작게 만드는 릿지(Ridge)회귀 해결 2. 예측 영향력이 작은 피처의 회귀 계수를 0으로 만들어 해당 피처가 아예 예측에 사용되지 않도록 하는 라쏘(Lasso) 회귀 해결 3. 릿지와 라쏘 두 가지 방법을 결합해서 사용하는 엘라스틱 넷(Elastic Net) 회귀 회귀의 유형 구분 내용 회귀명 독립 변수의 개수 한 개 단순 회귀(Simple) 독립 변수의 개수 두 개 이상 다중 회귀(Multiple) 예측 함수의 형태 선형 선형 회귀(Linear) 예측 함수의 형태 비선형 비선형 회귀(Linear) 규제의 유무와 형태 없음 일반 선형 회귀(General) 규제의 유무와 형태 L2 릿지 회귀(Ridge) 규제의 유무와 형태 L1 라쏘 회귀(Lasso) 규제의 유무와 형태 L1, L2결합 엘라스틱 넷 회귀(Elastic Net)","link":"/2023/02/22/ML-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%A0%95%EB%A6%AC/"},{"title":"금융공학 모델링 및 데이터사이언스","text":"금융데이터분석의 목표와 도구1. 모델링 f라는 함수 또는 처리도구를 만드는 것 현상 또는 특성을 수학적/정량적으로 표현 모델링으로 현실을 재현/대응 또는 어떠한 현상의 “근거”가 되게 함 2. 목적 및 필요성 인간의 뇌의 한계, 현실세계를 쉽게 이해하기 위한 추상화 금융공학에서는 고도화 -&gt; 수학, 컴퓨터 사용이 필수 재무 모델링 엑셀 기업 과거성과 input 기업 미래 성과 output 한정된 분야, 느린속도 금융공학 모델링 엑셀, c++, python 과거 성과 재무제표를 포함한 비지니스 전반의 데이터 미래 재무성과, 투자전략, 시뮬레이션 모든분야, 빠른속도 3. 방향 금융공학 목표 -&gt; 리스크 최소화, 재무성과예측 -&gt; 시장 움직임 분석 -&gt; 기초자산 및 내부변수 움직임 분석 -&gt; 불확실성 데이터화 및 모델링 -&gt; 확률론적 이해 및 수리통계 위는 섬세하게 논리를 좁혀옴 데이터분석 기획 &amp; 학습 방향은 위의 순서와 반대로 진행함 4. 도구 도구가 강력해져서 위에 쌓아놓은 지식을 몇줄로 처리할 수 있음 우리가 배우는 것들이 어려워 보이고 부담은 있지만 엄청 강력하다는 것을 알아야 함 빅데이터 및 데이터사이언스 시대 “인터넷과 정보통신, 그리고 관련 기기장치들의 발달로 인간이 남기는 모든 것들이 데이터로 측정되어 저장되는 시대” 1. 비지니스와 시장의 변화 과거 사후적으로 평가 월간 CPI, 분기 실적 등등을 누군가가 정리해야지 알 수 있음 신뢰성이 감소, 효용이 체감되지 않음 ex) 물가지수(미리 체감), 리서치들의 기업 이슈들(인터넷으로 미리 알 수 있음) 수학 방정식으로 특정한 입력에서 특정한 출력만 가져올 수 있음 현재 수많은 모델링으로 확장성이 큰 결과들을 만들어낼 수 있음 대체 데이터를 통해서 실시간 비지니스 정보 반영 가능 인간의 직관을 넘어서 수학과 컴퓨터의 결합으로 ML/DL이 빠르고 의미있는 결과를 출력함 2. 데이터사이언스 반영을 위한 2가지 재료(빅데이터) 사오고 다듬고 등등 해야함 레시피에 맞는 재료로 만들어야함 레시피보다 재료 손질이 사실상 훨씬 힘듦 데이터 준비는 가이드라인이 없고 방법도 늘 변화함 빅데이터 전처리, 가공 의지만 있다면 다양한 경로로 데이터 수집 가능 각종 기기장치에서 고객과 사회가 무의식적 행동 및 의사결정 변화 등을 스스로 제공 빅데이터를 사용할 수 있으려면 오랜기간 인내심이 필요함 레시피(통계/ML/DL) 분석방법론 데이터를 분석할 수 있는 능력 데이터가 많을수록 사람이 모든 데이터를 들여다 보기엔 시간이 너무 많이듦 경험/직관 -&gt; 데이터집계 -&gt; 통계추론 -&gt; ML/DL 통계추론 소수의 데이터를 통해서 다수의 정보를 추정 표본 특성 분석 -&gt; 일반화 여부 -&gt; 모집단 특성으로 추정 적은 비용과 시간 아무리 빅데이터 시대라서 데이터가 많더라도 아직은 모집단 크기의 데이터는 없음 그렇기 때문에 여전히 추론은 매우 중요함 ML/DL 방대하고 복잡한 데이터를 효과적으로 분석, 다양한 고객과 시장의 변화 패턴 포착 통계 : 소수에서 다수를 추정, ML : 과거에서 미래를 추론 3. 빅데이터, 인공지능, 데이터사이언스 입력 - 처리 - 출력 빅데이터 : 입력 인공지능 : 처리 모든 과정 : 데이터 사이언스 빅데이터 전문가 : 빅데이터를 수집, 처리, 관리, 저장 데이터사이언티스트 : 데이터를 분석, 시각화, 통찰력, 스토리텔링 등으로 설득을 진행 데이터 사이언스 어떤 것을 풀 수 있는가? 지도학습 정답이 있는 것 회귀, 분류 비지도학습 정답이 없는 것 군집, 차원변환 강화학습 정답과 상관없이 최적의 행동을 학습하는 것 내가 풀어야 할 문제를 제대로 정의하고 거기에 맞는 알고리즘을 선택함이 중요함 최신이라고 무조건 최고인 것은 아님 데이터 사이언스를 알아야 하는 이유1. 미래의 시장과 우리의 삶의 방식이 더 나은 방향으로 변화 새로운 형태의 산업혁명으로 기존의 구체제들은 몰락 데이터 없이는 어떤 의견이나 의사결정은 의미가 없어질 것 데이터 지식 습득이 필수조건이 된 세상 암기가 점점 더 의미가 없어지는 세상 2. 국가 및 기업의 발전 동력 퀀트 뿐만 아니라 펀더멘탈 투자자들에게도 ML은 필수적인 스킬 셋이 되어가고 있음 단기, 중/장기 분석도 기계로 넘어가고 있음 기계는 스크레핑, 시각적 의사결정도 매우 빠르고 지속적 3. 고임금직업 및 필요수요 파이썬이 제일 쉽고 돈도 많이 받음 4. 우리의 현실(두려움, 불편함, 위험?) 1950년대 월마트 창업자는 비행기에서 주차장 자동차의 갯수로 투자부지 선정 의사결정에 활용 현재는 파이썬으로 이를 할 수 있음 이렇게 생각해보면 대단히 새로운 것이 생긴 것이 아님 도구를 배우는 것은 동일하게 진행하지만 결국 문제를 해결하는 것은 통찰력이 필요한 일이고 창의적인 것임 너무 테크닉과 기교에만 집중하면 안된다는 것을 기억해야함 도구를 익히는 것과 문제를 해결하는 것은 전혀 다른 일 누가 ‘먼저’ 양질의 데이터를 찾고 분석해서 보석을 창출하는지는 “”선착순””","link":"/2023/02/21/%EA%B8%88%EC%9C%B5%EA%B3%B5%ED%95%99%20%EB%AA%A8%EB%8D%B8%EB%A7%81%20%EB%B0%8F%20%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4/"},{"title":"확률과 통계 강의 내용 정리","text":"확률, 통계, 및 데이터사이언스데이터분석과 수학/통계적언어 데이터 분석: 사람이 비이성적, 비효율적으로 처리하는 방식을 정량적, 체계적, 합리적 의사결정 하도록 다양한 도구를 활용하는 것 사람 현상 -&gt; 경험/직감/인터넷으로 짧게 공들여서 관찰 -&gt; 그 이유를 대고 주장 -&gt; 나와 의견이 같으면 좋고 아님 말고 문제의 해결은 대부분 없음 분석 현상을 보고 문제를 정의 -&gt; 이유를 정량적 체계적으로 추측(시각화, 기술적 분석, 상관분석) -&gt; 통계적 검정을 통해서 주장(통게추론, 확률적 검정) -&gt; 그 주장이 합리적이면 알고리즘으로 그 현상을 재현, 주장을 설명(확률통계, 기계학습, 딥러닝 등등) -&gt; 해결하고 미래에도 사용 확률, 통계, 데이터사이언스 배경 데이터 시각화 : 데이터 한눈에 확인, 통계로 계산하기 위해서 컴퓨터 사용 기술적 분석 : 전체 데이터의 특성을 숫자들로 확인 상관관계/인과관계 : 데이터의 관계를 확인 통계추론 : 일부 데이터로 전체를 알고 싶을 떄 알고리즘학습 : 과거 데이터로 미래를 알고 싶을 떄 가설검정(A/B test) : 진실과 가까운 의사결정을 할 떄 확률과 통계의 관계 따로 배워서 다는 것 같지만, 일반적으로 통계가 확률을 사용 확률은 모델로 데이터를 예측, 통계는 데이터로 모델을 예측 통계 -&gt; 10을 넣으면 나오는 값이 얼마일까 / 스마트폰 색상 재고량 추정 알고리즘 생성 확률 -&gt; 위의 나온 값의 확률이 얼마일까 / 스마트폰 색상 재고량 추정 기반 비지니스 예측 통계추론 : 일부 데이터로 전체 특성을 확인하고 이를 가정해서 확률적으로 전체를 예측 알고리즘학습: 전체 데이터로 과거패턴을 확인하고 이를 사용해서 확률적으로 미래를 예측 사회현상에서 시작된 확률 일상생활 전반에 나타나고 있는 현상들을 가능성을 수리통계적으로 표현 한 것 확률의 기원 역사적으로 도박에서만 사용 르네상스에서 확률론이 학문적으로 대두 경우의 수(모든 가능성을 파악하는 것 -&gt; 이게 목적인 것이 중요함) 순열 순서를 고려해서 늘어놓는데 자리가 한정적일 때 nPr = n!/r! 조합 순서를 고려하지 않고 늘어놓을 때 nCr = nPr/r! 시간의 관점으로 봤을 떄 현상만 존재하냐 시간의 순서도 포함해서 고려하냐의 차이임 확률 및 확률분포 경우의 수를 배우는 이유는 실제 사건 또는 데이터의 모든 가능한 상황을 알기 위함 우리의 관심은 전체 케이스 중에서 특정 사건이나 값 -&gt; 확률 P(X) = x의 경우/전체경우 x는 확률변수 -&gt; 사건을 변할 수 있는 값이라는 의미 이 확률 값들을 그린 것이 확률분포도 = 히스토그램 y = P(X) 를 일반화 시키면 함수가 되기에 확률 밀도 함수/확률 질량 함수 가 됨 대표적인 확률분포 많은 확률분포를 그려보니 몇개의 모델로 표현이 된다는 것을 알게 되어서 확률분포를 연구함데이터 분석에서는 특정 확률분포라고 가정하고 분석을 진행함 이항분포 &amp; 정규분포 이항분포 사례 동전을 던져서 n개가 나올 확률 윷놀이를 할 떄 특정한 경우의 수가 나올 확률 사망률이 30%인 전염병에 걸려서 내가 죽을 확률 일어날 확률 $\\times$ 일어나지 않을 확률 $\\times$ $nCx$ 기대값(Expectation) : 확률분포로 알게된 확률들로 결국 어떤 값이 나올 것인가? $E(X) = sum(x \\times f(x))$ 예측 기대오차(Variance) : 예측되는 확률변수 분포의 분산값, 예상되는 확률변수값이 기대값으로 얼마나 퍼져있는가? $Var(X) = E[(X-E(X))^2]$ 정규분포 어떤 시행을 여러번 반복하면 과연 어떻게 될까에서 시작됨 반복을 해보니까 대부분 정규분포라는 확률분포를 따르게 됨 60번 반복해서 주사위를 던졌을 떄 특정 값이 나올 확률이 얼마인가? 그럼 600번은? 60000번은? 그러다 보니 시행 횟수가 증가할수록 기댓값이 n * p 에 가까워짐 특징 정규분포는 -무한에서 +무한까지 값을 가짐 기댓값을 중심으로 종모양 확률변수가 무한이기에 확률 밀도 함수라고 부름 확률분포의 종류 이산형 : 확률변수의 값을 셀 수 있는 확륣분포 베르누이 이항 : 베르누이를 여러번 실행해서 1회 성공 음이항 : 베르누이를 여러번 실행해서 r회 성공 기하 : 베르누이를 여러번 실행해서 첫번째 성공이 나타날 때까지 실패할 횟수 다항 : 확률변수 값이 3가지 값만 발생하는 경우의 분포 연속형 : 확률변수의 값을 셀 수 없는 확률분포 포아송: 이항분포를 연속형으로 변환, 특정 시간/장소에 나타나는 희귀사건 개수의 분포 균등: 확률변수 값이 모두 균등 정규분포: 평균과 분산이 특정 값을 가지는 종모양의 분포 표준정규분포: 평균과 분산이 0,1로 고정된 종모양의 분포 지수: 포아송분포에서 1개의 희귀사건이 발생 할 때까지의 대기시간 분포 감마: 포아송분포에서 a개의 희귀사건이 발생 할 때까지의 대기시간 분포 t분포: 데이터의 개수가 적을 떄 표준정규분포보다 꼬리가 다소 두터운 종모양의 분포, 두 집단의 평균이 동일한지 비교할 떄 사용 F분포: 자유도가 커질수록 정규분포에 가까워지는 분포, 두 집단의 분산이 동일한지 비교할 때 주로 사용 카이제곱분포: 표준정규분포를 따르는 확률변수의 제곱합의 분포, 샘플과 전체의 분산이 동일한지 비교할 때 주로 사용 확률분포의 관계 확률분포를 보는 것은 내 데이터가 어떤 확률분포인지 확인하기 위한 것 특정 분포를 가정하고 그에 따른 평균이나 분산을 구할 수 있기 떄문에 유용함 전처리의 방식이 변화될 수 있기 떄문에 파악하는 것은 매우 중요함 확률분포를 가정하는 것은 결국 모델링을 진행해보는 것임 데이터의 분포 예측 및 검증 통계적 분포 모델링 어떤 종류의 현상을 관찰한 것인지 또는 데이터의 형태를 통해서 이미 아는 확률분포 가정/모델링 히스토그램을 통해서 가장 비슷한 분포를 가정/모델링 하지만 눈으로 봐서는 알 수가 없음 그래서 kolmogorov-Smirnov Test를 사용함 모델과 데이터의 분포차이가 비슷할수록 가장 큰 p-value를 반환 누적 했을 떄 가장 비슷하면 높기에 가장 높은 분포를 사용 정규분포 일반적으로는 정규분포 독립이 아니어서 정규분포가 아닐 수도 있음 다양한 확률분포일 가능성이 있음 독립이 아니라면 비모수 확률분포 또는 베이지안 확률을 사용하여 분석 조금의 변환을 이용하면 정규분포에 근사되도록 유도가 가능함 정규분포 = 가우시안분포 모수적 모델 vs 비모수적 모델 vs 세미모수적 모델 통계는 일반적으로 모수적 모델, ML/DL은 비모수적 모델이 대부분 모수적 모델 특정 확률분포를 기반하여 모수 추정 제한적 성능과 복잡성의 한계 비모수적 모델 모수의 형태를 확률분포로 가정하지 않고 주어진 데이터에서 직접 확률 계산 KNN, RandomForest 등 오버피팅의 문제가 있음 세미모수적 모수가 존재하지만 확률분포를 가정하지 않음 SVM, NN 등 반복되는 사건의 확률분포 sample에서 특정 분포를 따를 때 population 에서 특정 분포를 따른다고 생각할 수 있을까? 반복되는 동전던지기의 확률분포?(이산형) 100번 던졌을 때의 평균과 분산을 10, 50, 100, … 할 때 커질수록 정규분포와 비슷해짐 평균은 0.5에 가까워짐, 분산은 정규분포의 분산값 정도로 확률의 오차가 발생함(0에 가까워짐) 100번 던졌을 때 구해진 평균이 모집단의 평균과 같다는 것을 알 수 있음 위와같은 프로세스로 진행되기에 샘플로 모집단의 분포를 가정할 수 있음 연속형 확률변수의 샘플의 확률분포는 어떻게 될 것인가?(위와 같은 프로세스로 흘러갈 것인가?) 카이제곱의 경우에도 100번 추출하는 것을 100번 시행하니까 샘플의 평균과 비슷하게 됨을 볼 수 있음 중심극한정리 샘플들의 평균들을 구하면 전체 데이터의 특성을 알 수 있고 이 평균들의 분포가 정규분포 샘플들의 평균을 반복적으로 추정하면 평균 오차가 줄어들고 평균 정확성이 늘어남 하지만 전체 데이터의 분포 형태까지 추론하지는 못함 실험을 많이 할수록 의사결정 근거가 확실해짐 현실 확률을 다루는 방법 현실에서 확률을 다루는 방법: 투자에서 확률 및 통계이론에 근거한 투자를 함 위에서 배운 확률들은 빈도주의 확률 빈도주의 확률은 여러 이벤트는 독립을 가정하기에 여러번의 독립사건이 누적되어도 단순 곱하기로 계산됨 다른 분야는 베이지안 확률이 있음 모든 사건은 비독립이라서 서로 영향을 주고 여러 이벤트는 베이지안의 곱으로 확률 계산 빈도 확률은 데이터가 존재하거나 시도 가능한 경우에만 사용가능하고 데이터가 충분하지 않거나 시간 또는 비용이 너무 큰 사건들을 계산 불가 베이지안 : 새로운 정보가 발생함으로 기존의 정보와 추론 확률이 어떻게 영향을 받아 변하는지 표현 사전확률 : B 발생전 A 확률 사후확률 : B 발생 후 A 확률 증거 : 실제사건 B의 확률 = A가 발생하고, B가 발생 + A가 발생하지 않고, B가 발생 가능도 : A 발생시 B 확률 베이지안은 어떨 때 써야하는지만 알면 되는 것이다! 베이지안의 사례 평소 60%로 거짓말하는 사람이 있고, 90%의 거짓말 탐지기가 있을 때 이 사람이 하는 말이 거짓말일 확률은? 빈도주의 : 0.6 * 0.9 = 0.54 베이지안 : 거짓말 탐지기로 이 사람 말이 거짓말일 확률 증거: 거짓말 탐지기로 거짓말로 반정될 경우 = 0.6 * 0.9 + 0.4 * 0.1 = 0.58 가능도 :거짓말을 했을 때 거짓말 탐지기가 거짓이라고 할 확률 = 0.9 사전확률: 사람이 거짓말 할 확률 = 0.6 사후확률: 0.93 …?! 베이지안의 핵심: 관찰을 통해서 새로운 정보가 추가되면 확률이 업데이트 됨 데이터 종류 및 용어 데이터 관점에 따른 분류 횡단면 데이터 특정 시점의 여러가지 독립변수들을 정리한 데이터 시계열 데이터 다수시점 + 특정독립변수 다수 시점의 인구수 시계열 횡단면 데이터 다수시점 + 다수 독립 변수 시점/변수의 불일치로 공백이 있을 수 있음 패널 데이터 다수시점 + 다수 독립 변수 포함관계로 보면 패널이 시계열 횡단면 데이터에 포함 시점/변수 일치로 공백이 없음 이 데이터를 만들기 위해서 전처리를 해야함 매우 중요한 과정 데이터 변수구분 및 용어정리 원데이터(raw data, log, table etc..) : 처리되지 않고 순서화되지 않은 그대로 보존된 데이터 명목, 순서, 이산, 연속 변수 독립, 종속 변수 데이터 특성 확인을 위한 기술 통계 데이터가 매우 많을 때 데이터가 어떻게 생겼는지 알기 위함 중심 통계량 평균 평균이 같더라도 데이터의 전체적인 특성을 확인하기 위해서 필요한 것이 중앙값, 최빈값 두 값은 평균의 단점을 보완해줌 변동 통계량 범위 : 최댓값과 최솟값의 차이 편차 : 관측값과 평균의 차이 변동 : 편차 제곱의 합 분산 : 편차 제곱의 합을 데이터의 수로 나눈 값 형태 통계량 중심, 변동 통계량으로 데이터가 파악이 안될 때 사용함 왜도 : 평균을 중심으로 좌우로 데이터가 편향된 정도 첨도 : 뾰족함 정도 종목에 따라서 분포가 다르기 떄문에 포트폴리오를 만들 떄 리스크 관리를 할 수 있음 이상치 빅데이터에서는 이상치를 극단값으로 이해함 관계 통계량 : 위의 통계량은 각각 하나의 변수의 특성을 이해했지만 이 통계량은 각 변수들 간의 관계를 파악함 공분산 : 특정 값이 변화할 떄 감소 또는 증가하는지 확인 x1, x2의 각각의 기댓값에서의 차이를 곱한 값 단위 크기에 영향을 받아서 같은 결과의 시험이라도 100점 만점인 경우와 10점 만점인 경우의 공분산 값의 차이가 존재함 상관관계 : 공분산을 표준화 시킨 값 공분산을 x1의 분산과 x2의 분산을 곱한 값의 제곱근을 나눠준 값 이상치에 영향을 받고 선형관계만을 측정하기 떄문에 비선형인 데이터에는 부적절한 지표 인과관계 : 우리가 일반적으로 보는 것들은 상관관계일 뿐이지 인과관계는 거의 없음 통계를 이용한 조작 : 특정하게 skew된 sample을 수집하고 임의로 outlier를 정해서 값 변경할 수 있음 데이터를 최대한 비판적인 관점으로 바라볼 필요가 있음","link":"/2023/02/22/%ED%99%95%EB%A5%A0%EA%B3%BC-%ED%86%B5%EA%B3%84-%EA%B0%95%EC%9D%98-%EB%82%B4%EC%9A%A9-%EC%A0%95%EB%A6%AC/"},{"title":"boost 알고리즘(XGBoost, LightGBM, CatBoost)","text":"XGBoost Gradient Boost를 병렬학습이 가능하게 구현한 라이브러리가 XGBoost Regression, Classification 모두 지원하고, 성능과 자원 효율이 좋아서 인기가 많음 초기에는 사이킷런의 다양한 함수들과 연동이 안됐지만 sklearn 래퍼 XGB도 지원하고 있음 GBM과의 차이점 항목 내용 GBM 대비 빠른 수행시간 GBM이 순차적으로 학습하는 반면 XGB는 병렬처리가 가능하기에 빠르게 학습을 완료할 수 있음 과적합 규제(Regularization) 과적합 규제가 가능함으로 과적합에 좀 더 강한 내구성을 가짐 Tree Pruning 더 이상 긍정 이득이 없는 분할을 가지치기 해서 분할 수를 더 줄임 내장된 교차 검증 반복 수행 시마다 내부적으로 학습 데이터 세트와 평가 데이터 세트에 대한 교차검증을 진행하기에 최적화된 값을 가짐, 평가 값이 빠르게 최적화 되면 반복을 중간에 멈추는 조기 중단 기능이 있음 결손값 자체 처리 결손값을 자체 처리할 수 있는 기능이 있음 XGBoost의 하이퍼파라미터 일반 파라미터: 실행 시 스레드의 개수나 silent모드 등의 선택을 위한 파라미터로 디폴트 값을 바꾸는 경우는 거의 없음 booster default: gbtree gbtree(tree based) or gbliner(linear model) silent default: 0 출력 메시지를 나타내ㅑ고 싶지 않을 경우 1 nthread default: CPU 전체 스레드 사용 CPU의 실행 스레드 개수를 조정함 ML어플리케이션을 구동하는 경우에 변경함 부스터 파라미터: 트리 최적화, 부스팅, regularization 등과 관련된 파라미터 등 eta(learning_rate) default : 0.3 부스팅을 수행할 떄 업데이트 되는 학습률 값 보통은 0.01 - 0.2 사이의 값을 선호 num_boost_rounds n_estimators 와 같음 min_child_weight default: 1 gamma default : 0 리프노드의 추가분할을 결정할 최소손실 감소값 값이 높을수록 과적합이 방지됨 sub_sample default : 1 GBM의 subsample과 동일, 트리가 커져서 과적합 되는 것을 제어하기 위해서 사용함 일반적으로 0.5-1 사이의 값을 사용함 lambda default : 1 L2 규제 적용 값 피처가 많을수록 적용을 검토하며 값이 클수록 과적합 감소 효과가 있음 alpha default : 0 L1 규제 적용 값 피처가 많을수록 적용을 검토하며 값이 클수록 과적합 감소 효과가 있음 학습 태스크 파라미터 AdaBoost Adaptive Boost의 줄임말 오류 데이터에 가중치를 부여하면서 부스팅을 수행하는 알고리즘 Decision Tree를 약한 학습기로 주로 사용함 참고자료 공식문서 XGBoost AdaBoost https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor 블로그 https://wooono.tistory.com/97","link":"/2023/02/22/boost-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-XGBoost-LightGBM-CatBoost/"},{"title":"boosting 알고리즘(AdaBoost, GBM)","text":"부스팅 기법 여러 개의 머신러닝 기법을 차례대로 학습해 예측하고 잘못된 예측에 벌점을 주거나 가중치를 높여 오류를 개선해 가면서 학습하는 방법 대표적인 알고리즘 AdaBoost GBM(Gradient Boosting Machine) XGBoost LightGBM CatBoost AdaBoost Adaptive Boost의 줄임말 오류 데이터에 가중치를 부여하면서 부스팅을 수행하는 알고리즘 Decision Tree를 약한 학습기로 주로 사용함 AdaBoost의 학습 사진출처:https://sevillabk.github.io/Boosting/ 약한 학습기로 훈련해서 +,-를 분류 오류 데이터에 대해서 가중치 값을 부여, 여기서는 +에 가중치를 부여 두 번째 약한 학습기가 분류 기준2로 +,-를 분류 잘못 분류된 - 오류 데이터에 대해서 다음 약한 학습기가 잘 분류할 수 있게 가중치 부여 세 번째 약한 학습기가 분류 기준 3으로 +,-를 구분, 여기까지 순차적으로 오류값에 대해서 가중치를 부여한 예측 결정기준을 모두 결합해서 예측을 수행 모든 학습기를 결합한 결과 예측 AdaBoostClassifier의 실행 코드 : Adaboost.py12345678910111213141516171819import pandas as pdfrom sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.ensemble import AdaBoostClassifierfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.metrics import accuracy_scoreiris = load_iris()df = pd.DataFrame(iris['data'], columns = iris.feature_names)df['y'] = iris['target']x_train, x_val, y_train, y_val = train_test_split(df.drop(['y'], axis = 1), df['y'], test_size=0.2, random_state=42)ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42, learning_rate=0.01)ada.fit(x_train, y_train)pred = ada.predict(x_val)print('AdaBoost 정확도 : {:.4f}'.format(accuracy_score(y_val, pred))) AdaBoost의 하이퍼파라미터 base_estimators(1.2 버전에서는 base_estimator 에서 estimator로 변경됐다고 함) 학습에 사용하는 알고리즘 default는 DecisionTreeClassifier(max_depth = 1)이 적용 Regressor의 경우 DecisionTreeRegressor(max_depth = 3)이 적용 n_estimators boosting이 끝날 떄 까지 생성할 최대 estimator 개수 default는 50 (1,inf) 사이 값을 가짐 perfect fit을 가지면 더 일찍 끝날 수 있음 learning_rate 각 분류기에 적용될 가중치 높을수록 더 많은 기여를 함 learning_rate, n_estimators 는 trade-off 가 있음 (0.0, inf) 사이의 값 loss[Regressor only] loss function의 종류를 결정 default : linear [‘linear’,’square’,’exponential’] 중에 선택 Gradient Tree Boosting AdaBoost와 비슷하지만 가중치를 업데이트할 떄 경사하강법을 이용하는 것이 큰 차이점 분류의 실제 결괏값을 $y$, 피처에 기반한 예측 함수를 $F(x)$함수라고 하면 오류식 $h(x) = y - F(x)$가 되고 이를 최소화 하기 위해서 경사 하강법을 이용함 GradientBoostingClassifier의 실행 코드 : GBM.py123456789101112131415161718import pandas as pdfrom sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.ensemble import GradientBoostingClassifierfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.metrics import accuracy_scoreiris = load_iris()df = pd.DataFrame(iris['data'], columns = iris.feature_names)df['y'] = iris['target']x_train, x_val, y_train, y_val = train_test_split(df.drop(['y'], axis = 1), df['y'], test_size=0.2, random_state=42)gbm = GradientBoostingClassifier(n_estimators=200, random_state=42, learning_rate=0.01)gbm.fit(x_train, y_train)pred = gbm.predict(x_val)print('GBM 정확도 : {:.4f}'.format(accuracy_score(y_val, pred))) 위의 코드에서 확인할 수 있듯이 AdaBoost와 크게 차이나지 않음 GBM의 하이퍼파라미터 loss 경사 하강법에 사용할 비용함수를 지정함 특별한 이유가 없으면 기본값인 ‘deviance’를 그대로 적용 [‘log_loss’,’deviance’,’exponential’] n_estimators boosting이 끝날 떄 까지 생성할 최대 estimator 개수 default는 100 (1,inf) 사이 값을 가짐 perfect fit을 가지면 더 일찍 끝날 수 있음 learning_rate 각 분류기에 적용될 가중치 높을수록 더 많은 기여를 함, 하지만 너무 작으면 오래걸림 learning_rate, n_estimators 는 trade-off 가 있음 default는 0.1 (0.0, inf) 사이의 값 subsample 약한 학습기가 학습에 사용하는 데이터의 샘플링 비율 default는 1 -&gt; 모든 데이터를 다 학습한다는 의미 과적합이 염려되는 경우에 1보다 작은 값으로 설정 max_depth 트리의 최대 깊이 default는 3 깊이가 깊어지면 과적합될 수 있으므로 적절히 제어 필요 min_samples_split 노드를 분할하기 위한 최소한의 샘플 데이터수 → 과적합을 제어하는데 사용합니다. 값이 작을수록 분할노드가 많아져 과적합 가능성 증가 default는 2 참고자료 sklearn 공식문서 앙상블 https://scikit-learn.org/stable/modules/ensemble.html#adaboost GBM https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor AdaBoost https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor 블로그 https://sevillabk.github.io/Boosting/","link":"/2023/02/22/boosting-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98(AdaBoost,GBM)/"},{"title":"금융공학 의사결정","text":"데이터기반 의사결정 3가지 요소 “간단하지만 매우 중요하고 자주 놓치는 부분” 문제정의, 분석기획, 성능검증 여기서는 수동적이고 정형화된 절차를 넘어서 창의적인 영역임 현실에서는 일반적으로 데이터가 없음 y = f(x) 를 만들 떄 무엇이 x이고 무엇이 f이고 무엇이 y인지도 모름 실제 문제를 어떻게 정의 할 수 있는지는 아무도 정해놓은 것이 없기에 내가 만들어야할 영역 정확성을 높이는 것이랑 현실 문제를 해결하는 것은 전혀 다른 수준의 것 성능검증 지표를 무엇으로 할 것인가? 또한 매우 중요함 MSE 등의 책상에서의 지표를 넘어서 현실 비지니스에서 적용할 수 있는 지표를 알아내야함 재고를 예측하는 사례 -&gt; 재고가 부족하게 예측하면 절대 안되는 상황 3가지 요소의 우선순위 문제정의, 분석기획은 도메인도 중요하고 창의적인 영역 이 둘이 가장 중요함 우선순위: 문제정의 &gt; 분석기획 &gt; 성능검증 순서로 중요함 앞선 단계가 없으면 다 새로 만들어야하는 것임을 알아야 함 문제가 바뀌면 기획이 바뀌고 기획이 바뀌면 성능검증이 바뀜 문제정의: 현실에서 어떤 문제를 풀 것인가? 일상의 사소한 의사결정 까지도 모두 해결해야할 문제 분석기획: 풀어야 하는 문제를 데이터분석 과정에서 어떻게 “증명”할지 기획 문제정의가 없으면 기획을 할 수 없음 고민해야 하는 질문 10가지 + a 내가 풀어야 하는 문제가 무엇인지 명확하게 정의하고 제시할 수 있는가? 사람들은 그 문제를 어떻게 해결하고 하는가? 데이터로 그 문제를 해결하려면 어떤 데이터가 필요한가? 해당 데이터를 수집하기 위해서는 어떤 방식을 활용하는 것이 적절한가? 데이터 수집이 용이해서 바로 사용할 수 있는가? 어떤 고객을 대상으로 어떤 상황, 어떤 기간동안 수집? 데이터의양은 예상만큼 적절한가? 너무 많거나 적지 않은가? 정제된 데이터를 사용해서 문제를 해결하려면 어떤 실험을 기준으로 분석? 데이터분석 결과, 어떤 인사이트 또는 정답 후보가 예상 되는가? 데이터분석 결과는 실제 비지니스에 적용이 가능한가? 모든 사람들의 합의가 있는 결과인가? 이렇게 수동적인 이외의 것들을 고민하는 것이 사실 더 중요함 강의에서는 수동적인 것들을 어떻게 잘할 수 있는지를 알려줌 A/B Test: 확률적 의사결정을 위한 필수과정 A/B test 나머지 모든 것을 동일하게 두고 한가지만 변화를 줄 떄 어떻게 달라지는지를 확인하기 위함 -&gt; 인과관계를 끌어내려고 노력 정답을 모르는 상황에서 비교를 해서 선택을 할 수 있는 방법 거시적사례 어떤 버튼이 기부를 가장 증가시킬까? 어떤 컨텐츠가 기부를 증가시킬까? 어떤 방식의 화면 형태가 가입율을 높일까? 미시적사례 회귀분석시에 각각의 변수가 미치는 영향을 분석할 떄 T-test를 통해서 확인이 가능함 가설 설정: 분석기획 필수조건 3가지 상호배반적인 문제로 만들어야함 A, B가 겹치는 부분이 없게 분리해야함 ex) 양치기는 거짓말쟁이다!(주장) vs 양치기는 거짓말쟁이가 아니다!(대립) 이슈: 거짓말을 하는 것을 어떻게 정의할 수 있는가? 모든 양치기가 다 거짓말쟁이? 증명가능성 증명 가능한 것이거나 범위로 제시해야함 이슈: 모든 양치기를 다 알 수 없고 일부에서는 예외가 존재할 수 있음 이슈: 과거에 거짓말을 했을 수도 있음 현재 대한민국에 있는 양치기들은 일반적으로 거짓말하는 경향이 있지 않다 vs 그들은 일반적으로 거짓말하는 경향이 있다 구체적 현재 대한민국에 있는 양치기들은 일반인 대비 거짓말을 많이 하지 않는다 vs 현재 대한민국에 있는 양치기들은 일반인 대비 거짓말을 많이 한다 가설 검정: 분석기획 추론방법 통계추론 : 샘플을 분석하여 모집단의 특성을 추론하고 신뢰성을 검정하는 것 중심극한정리 : 샘플들의 평균들을 구하면 전체 데이터의 특성을 알 수 있고, 이 평균들의 분포는 정규분포 실험반복 횟수가 증가하면 평균은 같아지고 분산은 작아짐 가설검정 방법 3단계 가설 설정 귀무가설(대중주장, Null, H0) : 기각되기를 기대하는 가설 대립가설(나의주장, Alternative, H1) : 채택되기를 기대하는 가설 검정통계량 추정 및 유의수준 설정 검정통계량: 검증(Evalutaion)통계량, 1회의 추정치라서 점추정이라고도 함 일반적으로 두개의 차이로 설정함 귀무가설과 대립가설의 비교를 위한 것 차이가 없다면 0에 가까움 차이가 크다면 0보다 커짐 신뢰구간: 검정통계량을 여러 횟수로 추정한 범위로 구간추정이라고도 함 유의수준: 분석가가 설정한 오류 허용치 (주로 5%) 귀무가설이 참인데 귀무가설이 틀렸다고 주장하게 될 허용오류 최대치 의사결정 유의확률(p-value): 실험데이터에서 대립가설이 발생할 확률 ex) 유의수준이 5%인데 유의확률이 10%이면 대립가설과 귀무가설을 차이가 없는 정도니 귀무가설이 참이라고 판단함 ex) 유의수준이 5%인데 유의확률이 1%이면 대립가설과 귀무가설을 차이가 있으니 대립가설이 참이라고 판단함 실제 분석기획 및 추론 논문읽기(내 알고리즘의 성능은 좋은가?) 가설설정 귀무: 지금 존재하는 알고리즘의 정확성은 최대 80% 대립: 내가 만든 알고리즘의 정확성은 90% 검정 통계량 추정 및 유의수준 검정통계량: 지금까지 존재하는 알고리즘들로 나올 수 있는 정확성(1회성) 신뢰구간 : 정확성을 여러번 계산해서 히스토그램 또는 분포(반복성) 유의수준: 알고리즘 정확성이 80%이상 나올 수 있는 허용오류 의사결정 유의확률: 일반적인 알고르짐 정확성이 최대 80%인데, 나의 실험에서 정확성이 80%이상 관찰될 확률 (단측 검정) 나의주장 참: 허용오류가 5%인데, 나의 실험에서 80%이상 정확성이 관찰될 확률은 3%로 희박하니 나의 90% 정확성의 알고리즘은 훌륭함","link":"/2023/03/02/%EA%B8%88%EC%9C%B5%EA%B3%B5%ED%95%99-%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95/"}],"tags":[{"name":"pandas","slug":"pandas","link":"/tags/pandas/"},{"name":"회귀","slug":"회귀","link":"/tags/%ED%9A%8C%EA%B7%80/"},{"name":"통계","slug":"통계","link":"/tags/%ED%86%B5%EA%B3%84/"},{"name":"ML 알고리즘","slug":"ML-알고리즘","link":"/tags/ML-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98/"},{"name":"데이터 사이언스","slug":"데이터-사이언스","link":"/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4/"}],"categories":[{"name":"ML","slug":"ML","link":"/categories/ML/"},{"name":"금융공학 강의","slug":"금융공학-강의","link":"/categories/%EA%B8%88%EC%9C%B5%EA%B3%B5%ED%95%99-%EA%B0%95%EC%9D%98/"}],"pages":[]}